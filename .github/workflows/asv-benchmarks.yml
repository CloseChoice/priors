name: ASV Benchmarks

on:
  push:
    branches:
      - ci/benchmarking
      - main
  pull_request:
    branches:
      - ci/benchmarking
      - main
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for ASV

      - name: Restore ASV results cache
        uses: actions/cache@v4
        with:
          path: .asv/results
          key: asv-results-${{ github.ref_name }}-${{ github.sha }}
          restore-keys: |
            asv-results-${{ github.ref_name }}-
            asv-results-main-
            asv-results-

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install asv virtualenv

      - name: Configure ASV machine
        run: |
          mkdir -p .asv
          cat > .asv/asv-machine.json << 'EOF'
          {
            "github-actions": {
              "arch": "x86_64",
              "cpu": "GitHub Actions Runner",
              "machine": "github-actions",
              "num_cpu": "2",
              "os": "Linux",
              "ram": "7GB",
              "version": 1
            }
          }
          EOF

      - name: Run benchmarks
        run: |
          set -e

          echo "ðŸ” Checking for existing benchmark results..."

          # Strategy for incremental benchmarks:
          # 1. If results exist: Only benchmark NEW commits (since last run)
          # 2. If no results: Full benchmark from 0.1.0 release (5adfd57)
          # 3. Fallback: Always benchmark HEAD as minimum

          if [ -f .asv/results/benchmarks.json ] && [ -d .asv/results/*/  ]; then
            echo "âœ… Found cached results, running incremental benchmarks..."
            echo "ðŸ“Š This will only benchmark NEW commits since last run (much faster!)"

            # Try incremental benchmark
            if asv run NEW --show-stderr --quick; then
              echo "âœ… Incremental benchmark completed"
            else
              echo "âš ï¸  No new commits to benchmark, ensuring HEAD is benchmarked..."
              asv run HEAD^! --show-stderr --quick || true
            fi
          else
            echo "ðŸ†• No previous results found - running initial full benchmark"
            echo "ðŸ“¦ Benchmarking from v0.1.0 (5adfd57) onwards to avoid Python 3.10 compatibility issues"
            echo "â±ï¸  This will take ~5-10 minutes for the first run..."

            # Full benchmark from 0.1.0 release
            asv run 5adfd57..HEAD --show-stderr --quick || {
              echo "âš ï¸  Some commits failed to build, continuing..."
              # Ensure at least HEAD is benchmarked
              asv run HEAD^! --show-stderr --quick
            }
          fi

          # Verify HEAD was benchmarked
          echo "ðŸ” Verifying HEAD commit is benchmarked..."
          asv run HEAD^! --show-stderr --quick || echo "âš ï¸  HEAD already benchmarked"

          echo "âœ… Benchmark run completed!"

      - name: Generate HTML report
        run: |
          echo "ðŸ“Š Generating HTML report from benchmark results..."
          asv publish
          echo "âœ… HTML report generated in .asv/html/"

      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## ðŸ“Š Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f .asv/results/benchmarks.json ]; then
            # Count total benchmarks
            RESULT_FILES=$(find .asv/results -name "*.json" -not -name "benchmarks.json" | wc -l)
            echo "âœ… Generated $RESULT_FILES result files" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Show recent commits that were benchmarked
            echo "### Recent Benchmarked Commits" >> $GITHUB_STEP_SUMMARY
            git log -5 --oneline >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "ðŸ“ˆ View full results at: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ No benchmark results generated" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Create .nojekyll file
        run: touch .asv/html/.nojekyll

      - name: Deploy to GitHub Pages
        if: github.ref == 'refs/heads/ci/benchmarking' || github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: .asv/html
          force_orphan: true
          user_name: 'github-actions[bot]'
          user_email: 'github-actions[bot]@users.noreply.github.com'
          commit_message: 'Update benchmarks from ${{ github.ref_name }}'

      - name: Comment PR with benchmark results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const message = `## ðŸ“Š Benchmark Results

            Benchmarks have been run for this PR.

            ðŸ“ˆ View the full results at: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/

            The benchmarks compare performance across different dataset sizes and configurations.
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: message
            });
