name: ASV Benchmarks

on:
  push:
    branches:
      - ci/benchmarking
      - main
  pull_request:
    branches:
      - ci/benchmarking
      - main
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for ASV

      - name: Restore ASV results cache
        uses: actions/cache@v4
        with:
          path: .asv/results
          key: asv-results-${{ github.ref_name }}-${{ github.sha }}
          restore-keys: |
            asv-results-${{ github.ref_name }}-
            asv-results-main-
            asv-results-

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install asv virtualenv

      - name: Configure ASV machine
        run: |
          echo "Configuring ASV machine..."

          # Use asv machine command to register the machine non-interactively
          # This is the proper way to set up the machine in ASV
          echo -e "\n\n\n\n\n\n" | asv machine --yes || {
            echo "Warning: asv machine command failed, trying alternative approach..."

            # Fallback: Create machine config manually
            MACHINE_NAME=$(hostname)
            mkdir -p .asv
            cat > .asv/asv-machine.json << EOF
          {
            "$MACHINE_NAME": {
              "arch": "x86_64",
              "cpu": "GitHub Actions Runner",
              "machine": "$MACHINE_NAME",
              "num_cpu": "2",
              "os": "Linux",
              "ram": "7GB",
              "version": 1
            }
          }
          EOF
          }

          echo "‚úÖ ASV machine configuration complete"
          cat .asv/asv-machine.json 2>/dev/null || echo "Using asv's auto-generated config"

      - name: Run benchmarks
        run: |
          set -e

          echo "üîç Checking for existing benchmark results..."

          # Get machine name for ASV
          MACHINE_NAME=$(hostname)
          echo "Using machine: $MACHINE_NAME"

          # Set ASV to use the machine without interactive prompt
          export ASV_MACHINE="$MACHINE_NAME"

          # Strategy for incremental benchmarks:
          # 1. If results exist: Only benchmark NEW commits (since last run)
          # 2. If no results: Full benchmark from 0.1.0 release (5adfd57)
          # 3. Fallback: Always benchmark HEAD as minimum

          if [ -f .asv/results/benchmarks.json ] && [ -d .asv/results/*/  ]; then
            echo "‚úÖ Found cached results, running incremental benchmarks..."
            echo "üìä This will only benchmark NEW commits since last run (much faster!)"

            # Try incremental benchmark
            if asv run NEW --show-stderr --quick --machine="$MACHINE_NAME"; then
              echo "‚úÖ Incremental benchmark completed"
            else
              echo "‚ö†Ô∏è  No new commits to benchmark, ensuring HEAD is benchmarked..."
              asv run HEAD^! --show-stderr --quick --machine="$MACHINE_NAME" || true
            fi
          else
            echo "üÜï No previous results found - running initial full benchmark"
            echo "üì¶ Benchmarking from v0.1.0 (5adfd57) onwards to avoid Python 3.10 compatibility issues"
            echo "‚è±Ô∏è  This will take ~5-10 minutes for the first run..."

            # Full benchmark from 0.1.0 release
            asv run 5adfd57..HEAD --show-stderr --quick --machine="$MACHINE_NAME" || {
              echo "‚ö†Ô∏è  Some commits failed to build, continuing..."
              # Ensure at least HEAD is benchmarked
              asv run HEAD^! --show-stderr --quick --machine="$MACHINE_NAME"
            }
          fi

          # Verify HEAD was benchmarked
          echo "üîç Verifying HEAD commit is benchmarked..."
          asv run HEAD^! --show-stderr --quick --machine="$MACHINE_NAME" || echo "‚ö†Ô∏è  HEAD already benchmarked"

          echo "‚úÖ Benchmark run completed!"

      - name: Generate HTML report
        run: |
          echo "üìä Generating HTML report from benchmark results..."

          # Temporarily modify asv.conf.json to only track current branch
          # This avoids "fatal: bad revision" errors in CI
          CURRENT_BRANCH="${GITHUB_REF_NAME}"
          echo "Current branch: $CURRENT_BRANCH"

          # Backup original config
          cp asv.conf.json asv.conf.json.backup

          # Update config to only use current branch
          python3 << EOF
import json
with open('asv.conf.json', 'r') as f:
    config = json.load(f)
config['branches'] = ['$CURRENT_BRANCH']
with open('asv.conf.json', 'w') as f:
    json.dump(config, f, indent=2)
EOF

          # Generate HTML
          asv publish

          # Restore original config
          mv asv.conf.json.backup asv.conf.json

          echo "‚úÖ HTML report generated in .asv/html/"

      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## üìä Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f .asv/results/benchmarks.json ]; then
            # Count total benchmarks
            RESULT_FILES=$(find .asv/results -name "*.json" -not -name "benchmarks.json" | wc -l)
            echo "‚úÖ Generated $RESULT_FILES result files" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Show recent commits that were benchmarked
            echo "### Recent Benchmarked Commits" >> $GITHUB_STEP_SUMMARY
            git log -5 --oneline >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "üìà View full results at: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è No benchmark results generated" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Create .nojekyll file
        run: touch .asv/html/.nojekyll

      - name: Deploy to GitHub Pages
        if: github.ref == 'refs/heads/ci/benchmarking' || github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: .asv/html
          force_orphan: true
          user_name: 'github-actions[bot]'
          user_email: 'github-actions[bot]@users.noreply.github.com'
          commit_message: 'Update benchmarks from ${{ github.ref_name }}'

      - name: Comment PR with benchmark results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const message = `## üìä Benchmark Results

            Benchmarks have been run for this PR.

            üìà View the full results at: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/

            The benchmarks compare performance across different dataset sizes and configurations.
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: message
            });
