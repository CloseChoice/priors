name: ASV Benchmarks (Reusable)

on:
  workflow_call:
    inputs:
      deploy_to_pages:
        description: 'Deploy results to GitHub Pages'
        required: false
        type: boolean
        default: false
      cache_key_prefix:
        description: 'Prefix for cache key (e.g., test, prod)'
        required: false
        type: string
        default: 'asv'

permissions:
  contents: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for ASV

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install asv virtualenv

      - name: Restore ASV results cache
        uses: actions/cache@v4
        with:
          path: .asv/results
          key: ${{ inputs.cache_key_prefix }}-results-${{ github.ref_name }}-${{ github.sha }}
          restore-keys: |
            ${{ inputs.cache_key_prefix }}-results-${{ github.ref_name }}-
            ${{ inputs.cache_key_prefix }}-results-main-
            ${{ inputs.cache_key_prefix }}-results-

      - name: Configure ASV machine
        run: |
          echo "Configuring ASV machine..."

          # Use asv machine command to register the machine non-interactively
          echo -e "\n\n\n\n\n\n" | asv machine --yes || {
            echo "Warning: asv machine command failed, trying alternative approach..."

            # Fallback: Create machine config manually
            MACHINE_NAME=$(hostname)
            mkdir -p .asv
            cat > .asv/asv-machine.json << EOF
          {
            "$MACHINE_NAME": {
              "arch": "x86_64",
              "cpu": "GitHub Actions Runner",
              "machine": "$MACHINE_NAME",
              "num_cpu": "2",
              "os": "Linux",
              "ram": "7GB",
              "version": 1
            }
          }
          EOF
          }

          echo "âœ… ASV machine configuration complete"

      - name: Run benchmarks
        run: |
          set -e

          echo "ðŸ” Checking for existing benchmark results..."

          # Get machine name for ASV
          MACHINE_NAME=$(hostname)
          echo "Using machine: $MACHINE_NAME"

          # Set ASV to use the machine without interactive prompt
          export ASV_MACHINE="$MACHINE_NAME"

          # Strategy for incremental benchmarks:
          # 1. If results exist: Only benchmark NEW commits (since last run)
          # 2. If no results: Full benchmark from 0.1.0 release (5adfd57)
          # 3. Fallback: Always benchmark HEAD as minimum

          if [ -f .asv/results/benchmarks.json ] && [ -d .asv/results/*/  ]; then
            echo "âœ… Found cached results, running incremental benchmarks..."
            echo "ðŸ“Š This will only benchmark NEW commits since last run (much faster!)"

            # Try incremental benchmark
            if asv run NEW --show-stderr --quick --machine="$MACHINE_NAME"; then
              echo "âœ… Incremental benchmark completed"
            else
              echo "âš ï¸  No new commits to benchmark, ensuring HEAD is benchmarked..."
              asv run HEAD^! --show-stderr --quick --machine="$MACHINE_NAME" || true
            fi
          else
            echo "ðŸ†• No previous results found - running initial full benchmark"
            echo "ðŸ“¦ Benchmarking from v0.1.0 (5adfd57) onwards to avoid Python 3.10 compatibility issues"
            echo "â±ï¸  This will take ~5-10 minutes for the first run..."

            # Full benchmark from 0.1.0 release
            asv run 5adfd57..HEAD --show-stderr --quick --machine="$MACHINE_NAME" || {
              echo "âš ï¸  Some commits failed to build, continuing..."
              # Ensure at least HEAD is benchmarked
              asv run HEAD^! --show-stderr --quick --machine="$MACHINE_NAME"
            }
          fi

          # Verify HEAD was benchmarked
          echo "ðŸ” Verifying HEAD commit is benchmarked..."
          asv run HEAD^! --show-stderr --quick --machine="$MACHINE_NAME" || echo "âš ï¸  HEAD already benchmarked"

          echo "âœ… Benchmark run completed!"

      - name: Store results as artifact
        uses: actions/upload-artifact@v4
        with:
          # Use cache_key_prefix instead of branch name to avoid / in artifact name
          # Branch names like ci/benchmarking contain / which is not allowed
          name: asv-results-${{ inputs.cache_key_prefix }}-${{ github.sha }}
          path: .asv/results/
          retention-days: 90

      - name: Generate HTML report
        run: |
          echo "ðŸ“Š Generating HTML report from benchmark results..."

          # For multi-branch support: Keep all branches in config
          # ASV will handle missing branches gracefully
          asv publish

          echo "âœ… HTML report generated in .asv/html/"

      - name: Create .nojekyll file
        run: touch .asv/html/.nojekyll

      - name: Deploy to GitHub Pages
        if: inputs.deploy_to_pages == true
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: .asv/html
          keep_files: true  # Keep files from other branches
          user_name: 'github-actions[bot]'
          user_email: 'github-actions[bot]@users.noreply.github.com'
          commit_message: 'Update benchmarks from ${{ github.ref_name }}'

      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## ðŸ“Š Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f .asv/results/benchmarks.json ]; then
            # Count total benchmarks
            RESULT_FILES=$(find .asv/results -name "*.json" -not -name "benchmarks.json" | wc -l)
            echo "âœ… Generated $RESULT_FILES result files" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Show recent commits that were benchmarked
            echo "### Recent Benchmarked Commits" >> $GITHUB_STEP_SUMMARY
            git log -5 --oneline >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            if [ "${{ inputs.deploy_to_pages }}" == "true" ]; then
              echo "ðŸ“ˆ View full results at: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/" >> $GITHUB_STEP_SUMMARY
            else
              echo "ðŸ“¦ Results stored as artifact (no deployment)" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âš ï¸ No benchmark results generated" >> $GITHUB_STEP_SUMMARY
          fi
