# Priors: High-Performance Frequent Pattern Mining in Rust

**Priors** (pronounced "prio-rs") is a high-performance frequent pattern mining library implemented in Rust with Python bindings. It provides both traditional and streaming algorithms for discovering frequent itemsets in large datasets, with a focus on memory efficiency and scalability.

## üöÄ Key Features

### **üî• Parallel FP-Growth Algorithm**
- **2-6x faster** than MLxtend and efficient-apriori on medium-large datasets
- **Multi-core parallel processing** using Rayon
- **Memory-optimized** flat array storage for reduced overhead
- **SIMD vectorized operations** using ndarray

### **üåä Streaming Lazy FP-Growth**
- **Process datasets larger than RAM** using streaming two-pass algorithm
- **Constant memory usage** regardless of dataset size
- **Configurable chunk sizes** for optimal memory/performance balance
- **100% pattern correctness** - identical results to regular FP-Growth

### **üìä Advanced Algorithms**
- **DERAR** (Dynamic Extracting of Relevant Association Rules) with statistical filtering
- **Bit vector optimization** for small datasets (‚â§64 items)
- **Traditional Apriori** implementation for comparison

## üì¶ Installation

```bash
# Clone the repository
git clone <repository-url>
cd priors/priors

# Install in development mode
pip install -e .

# Or build with maturin
pip install maturin
maturin develop --release
```

### Requirements
- Python 3.7+
- Rust 1.70+ (for building from source)
- NumPy

## üéØ Quick Start

### Regular FP-Growth (Fast, In-Memory)

```python
import numpy as np
import priors

# Create transaction matrix (transactions √ó items)
transactions = np.array([
    [1, 1, 0, 0, 0],  # Transaction 0: items {0, 1}
    [1, 0, 1, 0, 0],  # Transaction 1: items {0, 2}
    [1, 1, 1, 0, 0],  # Transaction 2: items {0, 1, 2}
    [0, 1, 1, 1, 0],  # Transaction 3: items {1, 2, 3}
], dtype=np.int32)

# Mine frequent patterns (min_support = 50%)
patterns = priors.fp_growth(transactions, min_support=0.5)

# Results: list of 2D arrays, one per itemset size
for level, itemsets in enumerate(patterns, 1):
    print(f"Level {level}: {itemsets.shape[0]} itemsets of size {itemsets.shape[1]}")
    for itemset in itemsets:
        print(f"  {itemset}")
```

### Streaming Lazy FP-Growth (Memory-Efficient)

```python
from lazy_fp_growth import lazy_fp_growth_simple

# Same data, but processed in memory-efficient chunks
patterns = lazy_fp_growth_simple(
    transactions=transactions,
    min_support=0.5,
    chunk_size=1000  # Process 1000 transactions at a time
)

# Results are identical to regular FP-Growth!
```

### Streaming from Large Files

```python
from lazy_fp_growth import lazy_fp_growth_from_file

# Process CSV files larger than RAM
patterns = lazy_fp_growth_from_file(
    file_path="huge_dataset.csv",
    min_support=0.01,
    chunk_size=5000,
    binary_format=True  # CSV contains binary transaction matrix
)
```

### Custom Streaming Processing

```python
from lazy_fp_growth import LazyFPGrowthProcessor

# Manual control for database streaming, etc.
processor = LazyFPGrowthProcessor()

# Pass 1: Count item frequencies across chunks
for chunk in your_data_chunks:
    processor.count_pass(chunk)

# Determine frequent items
frequent_items = processor.finalize_counts(min_support=0.01)

# Pass 2: Build FP-tree across chunks
for chunk in your_data_chunks:  # Re-iterate through data
    processor.build_pass(chunk)

# Mine patterns
patterns = processor.mine_patterns(min_support=0.01)
```

## üèÜ Performance Comparison

### Speed Benchmarks (Regular FP-Growth)

| Dataset Size | MLxtend | Efficient-Apriori | **Priors FP-Growth** | Speedup |
|-------------|---------|-------------------|---------------------|---------|
| 10K √ó 50    | 2.45s   | 0.89s            | **0.35s**          | 2.5x    |
| 50K √ó 80    | 18.2s   | 4.12s            | **1.58s**          | 2.6x    |
| 100K √ó 100  | OOM     | 12.8s            | **4.51s**          | 2.8x    |
| 200K √ó 100  | OOM     | 28.1s            | **9.82s**          | 2.9x    |

*MLxtend fails with OOM (Out of Memory) on larger datasets*

### Memory Efficiency (Lazy vs Regular FP-Growth)

| Dataset Size | Regular Memory | Lazy Memory | Memory Savings | Time Overhead |
|-------------|----------------|-------------|----------------|---------------|
| 50K √ó 100   | 126 MB        | 41 MB       | **3.1x**       | 3.3x          |
| 200K √ó 150  | 438 MB        | 194 MB      | **2.3x**       | 3.6x          |
| 500K √ó 200  | 521 MB        | ~180 MB     | **2.9x**       | 4.1x          |

## üîß Advanced Features

### DERAR Algorithm (Statistical Rule Quality)

```python
# DERAR finds higher-quality rules using statistical measures
rules = priors.derar(
    transactions=transactions,
    min_support=0.01,
    min_confidence=0.5,
    # Statistical filtering thresholds
    min_mutual_information=0.1,
    min_stability_score=0.05,
    min_target_concentration=0.02
)

for rule in rules:
    print(f"Rule: {rule['antecedent']} ‚Üí {rule['consequent']}")
    print(f"  Support: {rule['support']:.3f}")
    print(f"  Confidence: {rule['confidence']:.3f}")
    print(f"  Mutual Info: {rule['mutual_information']:.3f}")
    print(f"  Stability: {rule['stability_score']:.3f}")
```

### Bit Vector Optimization (Small Datasets)

```python
# Automatically uses bit vectors for datasets ‚â§ 64 items
# Up to 2.8x speedup on small, dense datasets
patterns = priors.fp_growth_bitvector(transactions, min_support=0.1)
```

## üí° When to Use Which Algorithm

### üèÉ‚Äç‚ôÇÔ∏è **Regular FP-Growth** - Use When:
- Dataset fits comfortably in memory (< 50% of available RAM)
- Speed is critical
- Processing small to medium datasets
- Interactive data exploration

### üåä **Lazy FP-Growth** - Use When:
- Dataset approaches memory limits
- Processing streaming data from databases/files
- Memory efficiency is more important than speed
- Regular FP-Growth causes OOM errors
- Dataset size is unknown or very large

### üìà **DERAR** - Use When:
- Quality of association rules matters more than quantity
- Need statistical confidence in discovered patterns
- Working in domains requiring statistical validation
- Want to filter out spurious associations

## üõ†Ô∏è Architecture

### Core Components

```
priors/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ lib.rs              # Main library with Python bindings
‚îÇ   ‚îî‚îÄ‚îÄ lazy_fp_growth.rs   # Streaming FP-Growth implementation
‚îú‚îÄ‚îÄ lazy_fp_growth.py       # High-level Python interface
‚îú‚îÄ‚îÄ benchmarks/            # Performance testing suite
‚îî‚îÄ‚îÄ tests/                # Unit and integration tests
```

### Key Data Structures

- **FPTree**: Compact tree structure for pattern storage
- **FrequentLevel**: Memory-efficient flat storage for itemsets
- **LazyFPGrowth**: Streaming processor with two-pass algorithm
- **BitVector**: SIMD-optimized representation for small datasets

## üß™ Testing & Benchmarks

```bash
# Run basic tests
python -m pytest tests/

# Run performance benchmarks
python benchmarks/ultimate_benchmark.py

# Test memory efficiency
python demo_memory_efficiency.py

# Stress test with large datasets
python benchmarks/fp_growth_stress_test.py
```

## üìä Technical Details

### Algorithm Complexity
- **Time**: O(|DB| √ó |F|) where |DB| is dataset size, |F| is frequent items
- **Space**: O(|F|¬≤) for regular FP-Growth, O(|F| + chunk_size) for lazy
- **Parallelization**: Collect-and-merge strategy across conditional FP-trees

### Memory Layout Optimizations
- **Flat array storage**: Eliminates Vec overhead (24 bytes ‚Üí 0 per itemset)
- **SIMD vectorization**: Uses ndarray column operations
- **Cache-friendly access**: Sequential memory patterns for better performance

### Streaming Algorithm (Lazy FP-Growth)
1. **Pass 1 (Counting)**: Count item frequencies across all chunks
2. **Finalization**: Determine frequent items and their ordering
3. **Pass 2 (Building)**: Build FP-tree using predetermined ordering
4. **Mining**: Standard FP-Growth mining on the complete tree

## üöÄ Performance Optimization Roadmap

Based on profiling and algorithmic analysis, here are the key areas where the current FP-Growth implementation can be significantly improved:

### **High-Impact Performance Optimizations**

#### **1. Memory Management Improvements**
- **Object pooling** for vector reuse (currently cloning vectors in every recursive call)
- **In-place pattern building** with backtracking instead of frequent allocations
- **Memory-mapped conditional trees** for large datasets
- **Structure of Arrays (SoA)** layout for better cache locality

#### **2. Algorithmic Optimizations**
- **Incremental conditional FP-tree updates** instead of full reconstruction
- **Cached support counting** with prefix sum arrays for O(1) queries
- **SIMD support counting** for multiple items simultaneously
- **Pattern precomputation** for frequently occurring subtrees

#### **3. Data Structure Enhancements**
- **Flat hash tables** (hashbrown) replacing standard HashMap
- **Compressed sparse representations** for large sparse trees
- **Array-based indexing** for small item counts
- **Variable-length encoding** for compressed indices

#### **4. Advanced Parallelization**
- **Work-stealing scheduler** for uneven conditional tree sizes
- **Nested parallelism** with intelligent depth limits
- **Lock-free result collection** using atomic operations
- **NUMA-aware** memory allocation and processing

#### **5. Compiler Optimizations**
- **Profile-Guided Optimization (PGO)** for hot path optimization
- **CPU-specific SIMD** (AVX2/AVX-512) for vectorized operations
- **Link-time optimization** improvements
- **Custom allocators** for specific access patterns

### **Advanced Algorithm Variants**

#### **FP-Growth*** (Maximal Patterns)
```rust
// Skip non-maximal patterns during generation
// 50-80% reduction in result size for dense datasets
```

#### **Closed FP-Growth**
```rust
// Generate only closed frequent itemsets  
// Significant memory savings without information loss
```

#### **Top-K FP-Growth**
```rust
// Mine only the K most frequent patterns
// Early termination optimizations
```

### **Expected Performance Gains**

| Optimization Category | Expected Speedup | Implementation Effort |
|----------------------|------------------|---------------------|
| Memory optimizations | 1.3-1.5x | Low |
| Algorithmic improvements | 2-3x | Medium |
| SIMD operations | 1.5-2x | Medium |
| Advanced parallelism | 1.5-4x | High |
| **Combined optimizations** | **5-10x** | - |

### **Implementation Priority**

#### **Phase 1: Quick Wins** (High Impact, Low Effort)
1. Object pooling for vector reuse
2. Support counting cache
3. Flat hash tables (hashbrown)
4. PGO compilation

#### **Phase 2: Algorithmic** (High Impact, Medium Effort)  
5. Incremental conditional trees
6. SIMD support counting
7. Work-stealing parallelism
8. Memory layout optimization

#### **Phase 3: Advanced** (High Impact, High Effort)
9. FP-Growth* and closed patterns
10. Lock-free result collection
11. CUDA/GPU acceleration
12. Distributed processing

### **Profiling Hotspots**
Expected bottlenecks based on algorithmic analysis:
- **Conditional FP-tree construction**: ~40% of execution time
- **Vector allocations and cloning**: ~25% of execution time
- **Hash table lookups**: ~15% of execution time
- **Pattern result formatting**: ~10% of execution time

```bash
# Profile with perf
perf record --call-graph=dwarf python benchmarks/profile_bottlenecks.py
perf report

# Profile with cargo flamegraph
cargo install flamegraph
flamegraph -- python your_benchmark.py
```

**Target Goal**: Achieve **10-20x speedup over MLxtend** while maintaining memory efficiency advantages.

## ü§ù Contributing

Contributions are welcome! Priority areas for contribution:
- **Performance optimizations** from the roadmap above
- Additional mining algorithms (ECLAT, FP-Max, etc.)
- GPU acceleration with CUDA/OpenCL
- Distributed processing with Spark/Dask integration
- More statistical measures for DERAR

## üìú License

[Add your license here]

## üôè Acknowledgments

- Built with [PyO3](https://pyo3.rs/) for Rust-Python bindings
- Uses [Rayon](https://github.com/rayon-rs/rayon) for parallelization
- DERAR algorithm based on [Mart√≠nez-Ballesteros et al. (2024)](https://www.mdpi.com/2078-2489/16/6/438)

---

**Priors** makes frequent pattern mining accessible for datasets of any size, from small interactive analysis to large-scale data processing pipelines. Whether you need blazing speed or memory efficiency, Priors has the right algorithm for your use case.
